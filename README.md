# Interpreting-Deep-Learning-Models
Trying to create the list of papers for understanding the workings behind the deep/machine learning models


## Axiomatic Attribution for Deep Networks
Paper Link: https://arxiv.org/pdf/1703.01365.pdf        
Code bases:

a) https://github.com/ankurtaly/Integrated-Gradients

b) https://github.com/hiranumn/IntegratedGradients

c) https://github.com/PAIR-code/saliency

## A Unified Approach to Interpreting Model Predictions
Paper Link: https://arxiv.org/pdf/1703.01365.pdf 
Code bases:

a) https://github.com/slundberg/shap

## Learning Important Features Through Propagating Activation Differences (DeepLIFT)
Paper Link: https://arxiv.org/pdf/1704.02685.pdf 
Code bases:

a) https://github.com/kundajelab/deeplift

## Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization
Paper Link: https://arxiv.org/pdf/1610.02391.pdf 
Code bases:

a) https://github.com/ramprs/grad-cam

b) https://github.com/jacobgil/keras-grad-cam

## "Why Should I Trust You?": Explaining the Predictions of Any Classifier (LIME)
Paper Link: https://arxiv.org/abs/1602.04938 
Code bases:

a) https://github.com/marcotcr/lime

## Towards better understanding of gradient-based attribution methods for Deep Neural Networks (DeepExplain)
Paper Link: https://arxiv.org/abs/1711.06104
Code bases:

a) https://github.com/marcoancona/DeepExplain

## The Building Blocks of Interpretability

Link: https://distill.pub/2018/building-blocks/


## Deep Learning with Electronic Health Record (EHR) Systems

Link: https://goku.me/blog/EHR

## Interpretability via attentional and memory-based interfaces, using TensorFlow

Link: https://www.oreilly.com/ideas/interpretability-via-attentional-and-memory-based-interfaces-using-tensorflow

